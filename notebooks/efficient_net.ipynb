{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a pretrained model - EfficientNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use transfer learning to achieve both faster training and improved performance. In this notebook we train on the whole EfficientNet without freezing layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We highly recommend running this notebook on a GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"..\")\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from src.utils import calculate_metrics, seed_everything\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchvision.models import efficientnet_b3, EfficientNet_B3_Weights\n",
    "from src.loading import load_data, load_test_data\n",
    "from src.loading import load_data\n",
    "from src.train import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seeds for reproducibility\n",
    "seed_everything()\n",
    "\n",
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "<a name='s1'></a>\n",
    "## 1. Downloading the dataset\n",
    "\n",
    "Fetching the dataset should take around 4-5 minutes. Unzipping takes 20s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if folder 'data/' is does not exist, download the data\n",
    "if not os.path.exists(\"data/\"):\n",
    "    # Dropbox URL\n",
    "    dropbox_url = \"https://www.dropbox.com/scl/fi/sa14unf8s47e9ym125zgo/data.zip?rlkey=198bg0cmbmmrcjkfufy9064wm&dl=1\"\n",
    "\n",
    "    # File path where the .zip file will be saved\n",
    "    file_path = \"data.zip\"\n",
    "\n",
    "    response = requests.get(dropbox_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        message = \"Download successful. The file has been saved as 'data.zip'.\"\n",
    "    else:\n",
    "        message = \"Failed to download the file. Error code: \" + str(\n",
    "            response.status_code\n",
    "        )\n",
    "\n",
    "    print(message)\n",
    "\n",
    "    # Path to the downloaded .zip file\n",
    "    zip_file_path = \"data.zip\"\n",
    "\n",
    "    # Directory to extract the contents of the zip file, in this current folder\n",
    "    extraction_path = \"\"\n",
    "\n",
    "    # Unzipping the file\n",
    "    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extraction_path)\n",
    "\n",
    "    extraction_message = (\n",
    "        f\"The contents of the zip file have been extracted to: {extraction_path}\"\n",
    "    )\n",
    "\n",
    "    print(extraction_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data with improved preprocessing. Set the batch size according to your machine, here we tried to set it as high as possible, as long as the GPU/CPU has enough memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .jpeg files in the data folder\n",
    "PATH_IMAGES = \"data/images_keep_ar\"\n",
    "PATH_LABELS = \"data/labels/trainLabels.csv\"\n",
    "\n",
    "batch_size = 2  # default 8\n",
    "img_size = (400, 400)  # default size 120/120\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train and validation, 90-10 ratio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, validation_loader = load_data(\n",
    "    PATH_LABELS, PATH_IMAGES, img_size, batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a batch of images\n",
    "for images, labels in train_loader:\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(np.transpose(images[0], (1, 2, 0)))\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use a pretrained EfficientNet_B3 pretrained on the ImageNet dataset. This model was the best in our initial analysis so we decided to pursue and try to fine tune it in order to achieve maximum performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with the best available weights\n",
    "weights = EfficientNet_B3_Weights.DEFAULT\n",
    "model = efficientnet_b3(weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our initial results, all the layer should be frozen. For our secondary results we unfreeze the last three layers. However, for fine-tuning we trained on the whole network. Since our notebooks will be the latest version, all of the layers will be unfrozen when running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Freeze\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# replace the last layer\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.3, inplace=True),\n",
    "    nn.Linear(in_features=1536, out_features=5, bias=True),\n",
    ")\n",
    "\n",
    "# Unfreeze last x layers\n",
    "# parameters_to_train = []\n",
    "# # unfreeze the last layer\n",
    "# for param in model.classifier.parameters():\n",
    "#     param.requires_grad = True\n",
    "#     parameters_to_train.append(param)\n",
    "\n",
    "# for param in model.features[6].parameters():\n",
    "#     param.requires_grad = True\n",
    "#     parameters_to_train.append(param)\n",
    "\n",
    "# for param in model.features[7].parameters():\n",
    "#     param.requires_grad = True\n",
    "#     parameters_to_train.append(param)\n",
    "\n",
    "# for param in model.features[8].parameters():\n",
    "#     param.requires_grad = True\n",
    "#     parameters_to_train.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "MODEL = \"results/models/eff_net_400x400.pt\"\n",
    "checkpoint = torch.load(MODEL)\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially we went with loss 0.001. For further fine-tuning we lowered the loss, as this proved to be more stable when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=3e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    validation_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    device,\n",
    "    model_name=\"results/models/eff_net_400x400_undersampling.pt\",\n",
    "    num_epochs=num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is for generating a submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model path\n",
    "MODEL = \"results/models/eff_net_400x400.pt\"\n",
    "\n",
    "# Initialize model with the best available weights\n",
    "weights = EfficientNet_B3_Weights.DEFAULT\n",
    "model = efficientnet_b3(weights=weights)\n",
    "\n",
    "# Replace the last layer\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.3, inplace=True),\n",
    "    nn.Linear(in_features=1536, out_features=5, bias=True),\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (400, 400)\n",
    "batch_size = 10\n",
    "test = load_test_data(\"data/test/\", img_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loop\n",
    "model.eval()\n",
    "test_preds = []\n",
    "test_names = []\n",
    "with torch.no_grad():\n",
    "    for images, names in tqdm(test):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        predicted = outputs.argmax(dim=1)\n",
    "        test_preds.extend(predicted.cpu().numpy())\n",
    "        test_names.extend(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make submission csv, first column \"image\" second columns \"level\"\n",
    "submission = pd.DataFrame({\"image\": test_names, \"level\": test_preds})\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
